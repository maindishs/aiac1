{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1주차과제jyp.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPHjhQ3kSjNUoE/fpVEP5lQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maindishs/aiac1/blob/master/1%EC%A3%BC%EC%B0%A8%EA%B3%BC%EC%A0%9Cjyp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RCksMeRvLfE",
        "colab_type": "text"
      },
      "source": [
        "__인공지능 사례 분석__\n",
        "=====   \n",
        "\n",
        ">각 분야의 제품 또는 서비스 분석     \n",
        "\n",
        "\n",
        "#1.언어\n",
        " + 구글 번역기   \n",
        "\n",
        "#2.음성\n",
        " + SKT 'NUGU'\n",
        "   + 네이버 클로바 'NEST'   \n",
        "\n",
        "#3.이미지\n",
        " + 페이스북 딥페이스   \n",
        "\n",
        "#4.자율주행\n",
        " + 테슬라 자율주행  \n",
        "\n",
        "#5. 학습목표 \n",
        "-----\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGpUBn7bbtum",
        "colab_type": "text"
      },
      "source": [
        "초기 자동번역 또는 기계번역\n",
        ">초기 기술은 크게__규칙기반__(RBMT, Rule-Based Machine Translation)과 __통계기반__(SMT, Statistical Machine Translation), 이를 합한 __하이브리드__(RBMT+SMT) 기반으로 구분된다.\n",
        " + RBMT =~ '기호주의 인공지능' :   \n",
        "> 문법 규칙이 아닐 경우에는 번역 오류가 상당히 높다.   \n",
        "> 개발자가 일일히 규칙을 입력 해야한다는 문제점이 있다.   \n",
        " + SMT :   \n",
        "> 통계기반은 딥러닝과 빅데이터를 활용하기 때문에 언어 데이터베이스 확보가 중요하다   \n",
        "> 단어와 구(Phrase) 형식으로 각각 나눠 번역해 조합하는 방식   \n",
        "\n",
        "\n",
        " **1.언어 : 구글 번역기**\n",
        "===\n",
        "\n",
        "# 구문 기반 기계번역(PBMT) -> 구글 신경망 기계번역(GNMT) -> + 제로샷 번역\n",
        "\n",
        "## 구문 기반 기계번역(PBMT)\n",
        "\n",
        "> 2007년 구글이 처음으로 번역기를 출시했을 때는 __'구문 기반' 기계번역(PBMT)알고리즘을__ 사용한 기술을 사용했었다. \n",
        "\n",
        "* 구문 기반 기계번역은 문장을 단어와 구 단위로 쪼개서 하나하나 개별적으로 번역하는 방식이다.\n",
        "\n",
        "  * 이 기술은 자주 사용되는 단어 중심의 번역 방식이다 보니 번역된 문장이 매끄럽지 못했다.\n",
        "\n",
        "   * 단순히 단어를 나열하는 수준의 번역을 제공해 오류가 많다는 지적이 있어왔다.   \n",
        "\n",
        "## 구글 신경망 기계번역(GNMT)   \n",
        "\n",
        ">2016년 9월 __'구글 신경망 기계번역(GNMT)'__ 기술을 처음으로 선보였다.   \n",
        "\n",
        "* 구글은 좀 더 인간의 언어와 비슷한 구조의 자연스러운 번역 서비스를 제공하기 위해 머신러닝을 도입한 인공신경망 기계번역 을 자체 개발했다.\n",
        "\n",
        "  * 인공신경망 기계번역은 단어를 개별적으로 번역하는 구문 기반과 달리, 전체 문장을 하나의 번역 단위로 간주해 한꺼번에 번역한다.   \n",
        "\n",
        "   * 이는 문장 전체의 맥락을 먼저 파악한 후 어순, 의미, 문맥별 의미 차이 등을 반영해 가장 적합한 문장으로 재배열하는 방식이다.   \n",
        "\n",
        ">>이로 인해 인공신경망 기계번역은 구문기반 기계번역보다 자연스러운 문장을 제공할 수 있으며, 전체 텍스트의 가독성 또한 향상됐다.\n",
        "\n",
        "\n",
        "## 제로샷 번역(Zero-Shot Translation)   \n",
        "\n",
        " * 구글은 더 다양한 언어에 신경망 기계번역 기술을 지원하기 위해 단일 시스템에서  여러 언어 간 번역이 가능하도록 하는 방식   \n",
        "\n",
        "  * 이는 다중 언어 트레이닝을 통해 실제 테스트하지 않은 여러 언어 조합의 번역도 데이터를 활용해서 가능하게 하는 기술이다   \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nzo1ut0DlF4V",
        "colab_type": "text"
      },
      "source": [
        ">__+ 신경망 번역 방식의 원리__\n",
        "\n",
        "* 입체 공간에‘먹다’라는 단어를 공간에 띄운다.   \n",
        " * 그리고 그 근처에 ‘먹었다’, ‘먹을 거다’, ‘먹고 싶다’ 등 ‘먹다’라는 단어와 관계가 있는 단어들을 유사한 공간에 둔다.   \n",
        "\n",
        "   * 이 ‘먹다’라는 단어에는 다양한 차원이 있을 수 있다. 이 차원에 따라 또 다른 단어들과 관계를 맺을 수 있다.   \n",
        "\n",
        "     + 예컨대 치킨, 피자, 케이크 등 ‘먹다’와 함께 쓰일 수 있는 단어들이 또 ‘먹다’와 나름의 관계를 맺고 공간상에 위치할 수 있다.   \n",
        "* 이렇게 단어나 구 등이 공간에서 관계를 맺으며 맵핑된다. \n",
        " * 이때 가지는 벡터값을 ‘단어 표현’이라고 한다. 번역기에 사용되는 단어는 200차원의 단어 표현 값으로 변환된다.\n",
        "\n",
        "* ‘나는’, ‘사과를’, ‘먹는다’, ‘I’, ‘eat’, ‘apple’은 각각 단어 표현 값으로 변환된다. \n",
        " * 그리고 이 단어 표현들을 이어가며 번역하려는 문장에서 결과 문장으로 이어주는 최적의 가중치(Weight parameter)들을 찾아 행렬 곱으로 이어가 벡터를 구해가는 방식이다.\n",
        "\n",
        "   * 여기서 번역하려는 문장과 결과 문장을 컴퓨터에 주고, 결과 문장이 나오게 하는 값을 찾아내는 최적의 가중치(WP)를 반복적인 기계학습을 통해 자동으로 컴퓨터가 학습한다. \n",
        "   * 번역은 EOS(문장의 끝, End Of Sentence)값이 가장 높아지면 끝난다. 번역 언어가 달라질 때마다 가중치 값이 바뀐다.   \n",
        "\n",
        " 이처럼 인공신경망 기계번역은 입력 문장과 출력 문장을 하나의 쌍으로 두고, 최적의 답을 찾는 중간 값을 학습한다.\n",
        "\n",
        ">인공신경망 기계번역 방식은 통계적 기계번역보다 번역 시스템이 단순하다는 장점을 가진다.   \n",
        ">* 입력 문장과 출력 문장만 있으면 알아서 학습하게끔 유도하기 때문에 구조 자체가 그렇게 어렵지 않기 때문이다.\n",
        ">* 인공신경망 기계번역은 확장하기 쉽고 다양한 구조를 채택할 수 있다는 것도 장점이다.   \n",
        " \n",
        "[출처](https://www.oss.kr/info_techtip/show/65f88554-1882-4af9-8036-6490d3314c65)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eeUBMDumIig",
        "colab_type": "text"
      },
      "source": [
        "**2.음성  : 스마트 스피커**\n",
        "===\n",
        " SKT 'NUGU' 음성 인식 기술\n",
        "---\n",
        "* 2014년부터 상용화 시작\n",
        "* 음성인식 기술 구성\n",
        ">음성인식 기술은 크게 모델을 학습하는 단계, 학습된 모델을 이용하여 인식하는 단계로 구분되고,\n",
        ">>이 중 음향, 언어 모델을 학습할 수 있는 기술이 핵심   \n",
        "\n",
        "* 통합모델(wFST) 기술\n",
        ">문장 단위 학습에 최적화되어 속도와 인식률 향상   \n",
        ">향후 대용량 연속 어휘, 즉 자연어 음성인식을 위한 핵심 기술   \n",
        "\n",
        "* 모델링 기술 및 데이터\n",
        "> 음향모델 : 입력 신호와 음소의 유사도   \n",
        "> 언어모델 : 단어간 확률 관계 그래프   \n",
        "> 발음사전 : 단어의 발성 정보 저장   \n",
        ">>예) 선릉 : 설릉(ㅅ ㅓ ㄹ ㄹ ㅡ ㅇ), 선능(ㅅ ㅓ ㄴ ㄴ ㅡ ㅇ)   \n",
        "\n",
        "* 음성인식 기술 구성 - 자체 기술 개발\n",
        ">속도, 성능을 향상시킨 wFST, 컴퓨팅 파워의 향상에 기반한 DNN 기술 적용\n",
        "\n",
        " * 1.전처리  \n",
        "   + Feature   \n",
        ">HLDA, STC, Equalization / Wiener, Kalman Filter / Model Space    \n",
        "\n",
        "   + Neural Net\n",
        ">Bottleneck Feature\n",
        "\n",
        " * 2.학습      \n",
        "   + Discriminative Training\n",
        ">MPE, fMPE   \n",
        ">MCE, MMI   \n",
        "\n",
        "   + Big LM\n",
        ">Distributed Modeling, Long Span LM   \n",
        "\n",
        "   + Deep Neural Network   \n",
        ">DNN based Acoustic Modeling Training   \n",
        "   \n",
        " + 3.인식   \n",
        "   + Dynamic Network   \n",
        ">FSN   \n",
        ">Lexical Tree   \n",
        "\n",
        "   + Static Network   \n",
        ">wFST (weighted Finite State Transducer)\n",
        "\n",
        "* 음성인식 기술 구성 - Neural Network 기반 언어모델링 (NN-LM)\n",
        ">Neural Net기반 언어모델에서는 아직까지 음향모델만큼 큰 성능향상을 이루지는 못함   \n",
        "\n",
        "* 음성인식 기술 구성 – Personalized Language Model (PLM)   \n",
        "> 개인화된 어휘가 사용될 위치를 사전에 class 형태로 모델링    \n",
        "\n",
        "* 음성인식 기술 구성 - Sequence to Sequence Learning, CTC \n",
        ">RNN-LSTM을 음절 기반의 띄어쓰기 모델에 확장 적용함으로써 성능 향상.\n",
        "발음열 생성기술에 CTC(Connectionist Temporal Classification) 적용 등 다양한 영역에서 DNN적용을 시도 중   \n",
        "\n",
        "[출처](https://docsplayer.org/108177235-Microsoft-powerpoint-_skt_jgjung-pptx.html)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQVve84QoPmk",
        "colab_type": "text"
      },
      "source": [
        " >**+네이버 클로바**\n",
        "===\n",
        "네이버가 개발한 음성인식 인공지능(AI) 엔진 'NEST'\n",
        "---\n",
        "* NEST는 뉴스·통화·미디어 음성 파일을 텍스트로 변환하는 데 최적화됐다.\n",
        "\n",
        "* 음향·언어모델 통합 학습하는 엔드투엔드 방식 적용 \n",
        "> * 엔드투엔드는 음향 모델(AM)과 언어 모델(LM)을 따로 학습하지 않고 통합해 학습하는 방식이다.   \n",
        "> * AM과 LM을 별도로 학습하는 기존 음성정보의 텍스트 변환 기술(STT)과는 차별화된다.   \n",
        "> * AM은 발음 정보를, LM은 어휘들의 변형 및 관계를 다루는데, 기존 STT에서 AM은 음성과 정답 데이터가 잘 정리된 데이터가 필요하고 LM은 복잡한 표현을 미리 학습해야 좋은 결과를 낼 수 있는 부담이 따랐다.   \n",
        "  \n",
        "* 데이터를 정제하는 부담이 줄었고 AI 엔진이 학습해야 하는 절대적인 양도 감소했다.   \n",
        "\n",
        "* 구어체 표현이나 비문에 일일이 미리 대응해두지 않아도 원래 음성과 유사한 인식 결과를 낼 수 있게 됐다.   \n",
        "\n",
        ">+ NEST 개발에는 네이버가 보유한 방대한 데이터도 한 몫했다.   \n",
        "\n",
        ">+ NEST 엔진은 수 분 혹은 수 시간 단위의 말 덩어리도 학습할 수 있도록 구현됐다.   \n",
        "\n",
        "> + 수 초 단위의 짧은 형태의 음성이 있어야 원활하게 학습할 수 있는 기존 엔진과 다른 점이다. \n",
        " \n",
        "[출처](http://www.newstomato.com/one/view.aspx?seq=967696)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBijhIK7nSd3",
        "colab_type": "text"
      },
      "source": [
        "**3.이미지 : 얼굴인식 (페이스북)**\n",
        "===\n",
        "#딥페이스\n",
        "\n",
        "페이스북 인공지능연구소는 텔아비브대 연구원과 함께 딥페이스 라는 얼굴인식 알고리즘을 개발했다.   \n",
        "딥페이스는 페이스북에 올라온 사진만으로도 어떤 사용자인지 찾아내는 기술이다.  \n",
        "\n",
        ">![움짤도 되나](https://miro.medium.com/max/1004/1*1cqoJbR49tytC36yZsLl-g.gif)   \n",
        "> 사진에 사람들에 대한 태그를 자동으로 달아줍니다.   \n",
        "   \n",
        "* 페이스북은 정확한 얼굴분석을 위해 동물의 중추신경계를 모방해 만든 신경망 분석을 활용했다.   \n",
        ">대량의 데이터 속 패턴을 인지하는 학습기능   \n",
        "\n",
        "* 딥페이스는 카메라에 비친 얼굴을 바탕으로 입체를 유추하면서 가상으로 얼굴을 회전시키는 3D모델을 활용해 얼굴 각도를 수정.   \n",
        "\n",
        "* 그 다음에는 유사한 이미지의 다른 사진에서 가상의 원형 얼굴을 비교하고,1억 2000만 가지 연관관계를 파악해내는 9개의 계층과 대조한다.   \n",
        "\n",
        "* 4000여 명의 사용자 얼굴에서 추출한 400만 개의 얼굴 이미지 데이터를 활용함으로써 사용자의 존재를 정확하게 찾아낸다.   \n",
        ">딥페이스의 정확도는 97.25%로 인간의 평균 눈 정확도(97.53%)에 가까운 수치다.\n",
        "\n",
        ">실제로 딥페이스는 사진에서 사람 얼굴이 어둡게 찍혀도,카메라에서 멀리 있거나 옆을 보고 있어도 가려낼 수 있다.   \n",
        "\n",
        "[출처][1]\n",
        "## 이미지 인식 기술의 발전   \n",
        "\n",
        "* 이미지 인식 기술은, 인식하고자 하는 대상을 포함하는 데이터를 수집하고 수집된 데이터를 통해 분류기를 학습하는, 이른바 머신러닝을 통해 발전했습니다.   \n",
        "\n",
        " * 이미지로부터 특징 정보를 추출하는 단계\n",
        "> HOG(Histogram Of Gradients), SIFT(Scale Invariant Feature Transform), LBP(Local Binary Pattern) 등은 대표적인 이미지 특징 정보   \n",
        "\n",
        " * 분류 모델을 통해 추출된 특징을 식별하는 단계로 구성되어 있습니다.\n",
        ">SVM(Support Vector Machine), AdaBoost(Adaptive Boosting), Decision Tree 등은 대표적인 분류 모델   \n",
        "   \n",
        "* 딥러닝의 신경망(Neural Network) 기반 분류 모델은, 불과 몇 년 전까지만 해도 극소 함정(Local Minimum), 과적합(overfitting) 등의 한계 때문에 SVM 등의 다른 분류 모델에 비해 크게 주목받지 못했습니다.   \n",
        " + 이미지 인식 기술의 기반이 되는 딥러닝 기술을 포함하여 머신러닝, 더 나아가 인공지능이 급격히 발전할 수 있었던 요인으로는   \n",
        "   * 신경망 모델의 발전\n",
        "> CNN(Convolutional Neural Network) 모델은 머신러닝의 '특징 추출, 분류' 의 두 단계를  -> '특징 추출과 분류' 의 한 단계로 통합했습니다.   \n",
        ">>특징 추출 문제조차 사람의 예측이 아닌, 데이터를 이용한 학습을 통해 접근\n",
        "   * 컴퓨팅 성능 향상\n",
        ">특히 GPU의 등장과 분산처리 환경의 발전은 동시에 많은 일을 할 수 있는 강력한 병렬 처리를 지원하게 되었습니다. \n",
        ">>이로 인해 예전에는 수일, 수개월이 걸렸던 학습 과정을 지금은 수시간, 수분 단위로 줄일 수 있게 되었습니다.\n",
        "   * 빅데이터의 등장과 활용 \n",
        "> 다양한 분야의 정보를 가진 대용량의 빅데이터를 학습에 사용함으로써 분류 모델은 과거처럼 정해진 분야에 한정되지 않고, 실제 환경에 일반적으로 적용할 수 있는 수준으로 발전할 수 있었습니다.   \n",
        "\n",
        "[출처][2]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "[1]:https://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=105&oid=008&aid=0003441998&cid=1011500&iid=1217209\n",
        "[2]:https://blog.ncsoft.com/%ea%b2%8c%ec%9e%84%ea%b3%bc-ai-10-%ec%9d%b4%eb%af%b8%ec%a7%80-%ec%9d%b8%ec%8b%9d-%ea%b8%b0%ec%88%a0%ec%9d%84-%ec%9d%b4%ec%9a%a9%ed%95%9c-%ec%95%bc%ea%b5%ac-%ed%95%98%ec%9d%b4%eb%9d%bc%ec%9d%b4/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbhWXKQMn-0-",
        "colab_type": "text"
      },
      "source": [
        "# **4.자율주행 : 테슬라 자율주행**\n",
        "테슬라는 운전 도움 기능들을 자동운행(Autopilot)과 완전 자율 주행(FSD;Full Self Driving Capability)로 나누어서 구분한다.  \n",
        "\n",
        "[테슬라 자율주행 기술]( https://www.tesla.com/autopilotAI )   \n",
        "\n",
        ">완전 자율 주행 기능   \n",
        "\n",
        "+ 자동 차선변경(Auto Lane Change)   \n",
        ">센서 혹은 측면 카메라로 옆차선의 공간을 확인하여 차선 변경\n",
        "+ 오토파일럿 내비게이션(고속도로;Navigation on Autopilot on the highway,)\n",
        ">고속도로 진입로에서 나들목, 진출로까지 자율주행\n",
        "+ 오토파크(Autopark)\n",
        ">평행주차, 직렬주차   \n",
        "* 차량 호출(Summon; 서몬)\n",
        ">주자창에서 무인 단거리 직진 전진, 후진\n",
        "* 스마트 호출(Smart Summon; 스마트 서몬, 스마트 차량 호출)\n",
        ">옥외 주차장에서 차주의 스마트폰 위치까지 무인 자율 저속 이동[9]\n",
        ">>스마트 호출은 옥외 주차장의 주행선 정보를 OpenStreetMap에서 사용하는 것으로 알려져 있다.   \n",
        "* 오토파일럿 내비게이션 (시내주행)\n",
        "  + 교통 신호등 및 일단 멈춤 표지판 인지(Traffic Light and Stop Sign Recognition)\n",
        ">2019년 말 HW3 자동차에서 출시   \n",
        ">2020년 4월 HW2/HW2.5 + MCU2 자동차에서 출시   \n",
        ">신호등, 정지선, 일단 멈춤 표지판,좌회전/우회전/기차건널목/마름모/자전거전용 도로 표면 표지 인식/시각화   \n",
        ">교통 장애물(대형쓰레기통, 공사콘, 공사드럼통) 인식/시각화   \n",
        ">장애인 주차 표식 인식/시각화   \n",
        "  + 교통 신호, 일단 멈춤 표지 인식 자율 정차(Traffic Light and Stop Sign Control)\n",
        ">2020년 4월 출시(미국)  \n",
        ">정지 신호와 일단 멈춤 표지를 인식하여 교차로에서 자율 정차   \n",
        ">설정된 경로상 우회전해야 하는 교차로에서 자율 정차   \n",
        ">건널목에서 자율 정차   \n",
        "\n",
        "그 밖에 교차로 정차후 직진, 신호 우회전, 신호 좌회전, 비보호 우회전, 비보호 좌회전, 회전교차로 통과등은 2020년 출시 예정\n",
        "* 스마트 오토파크(Smart Autopark)\n",
        ">주차장의 시간이나 사용 규칙등을 확인하여 빈공간을 무인으로 스스로 찾아서 주차하는 기능\n",
        "\n",
        "\n",
        "#Tesla\n",
        "* 비전 중심 접근 (Vision-based Approach)\n",
        " - 테슬라는 라이다(Lidar)와 HD 맵 없이 카메라만을 이용한 완전 비전 중심(heavily vision-based). 따라서 사전에 형성한 HD 맵 없이 처음 접하는 환경이더라도 카메라를 통해 차선, 신호등, 주변 차량들을 인식하여 주행   \n",
        "> 일반적으로는 먼저 자동차 위에 라이다를 올려 라이다를 통해 주변 환경에 대한 사전 지도(Pre-map)을 구현. 그런 다음 주행을 하며 이 맵을 로컬화시켜 자율주행의 정확도를 높이는 방법 이 쓰인다\n",
        ">>유일한 단점은 비싼 가격이다. 센서 하나당 비용이 8만 달러(약 9231만원)에 육박한다.\n",
        "\n",
        "* 테슬라 AI Stack\n",
        " - 테슬라는 전세계 약 100만대 가량 판매된 테슬라 차량들로부터 데이터 수집\n",
        " - 이를 GPU 클러스터 기반의 자체 개발한 맞춤형 하드웨어 및 소프트웨어를 통해 인공지능 신경망 훈련\n",
        " - 즉, 하드웨어부터 데이터 수집, 인공지능 설계 및 트레이닝까지 모든 과정을 자체 개발하여 내제화\n",
        "\n",
        "* 테슬라 자율주행 신경망 HydraNet\n",
        "   \n",
        " <img src=\"https://ifh.cc/g/JH0Zxa.png\" width=\"600px\" height=\"300px\" \n",
        "title=\"hybranet\" alt=\"background\"></img><br/>\n",
        " \n",
        " - 자율주행을 위해 도로, 사람, 차량, 신호, 표지판 등 분석해야 할 대상(태스크, Task)들이 거의 100개에 달하는데, 태스크마다 일일이 온전한 신경망을 만들기에는 여유가 없음. 따라서 각 신경망들이 백본을 공유하는 형태를 취하게 됨\n",
        "\n",
        "* 깊이 예측 & 멀티플 HydraNet\n",
        " - 인간의 경우 양쪽 눈으로 대상을 보는데 왼쪽 눈과 오른쪽 눈, 해당 사물 이 세 개가 삼각형 구조를 이룸. 뇌는 이렇게 각 눈에서 오는 정보를 통합하여 대상의 멀고 가까움을 인지\n",
        " - 도로의 윤곽을 파악하기 위해 테슬라는 차량에 달린 8대의 카메라, 즉 8개의 HydraNet으로부터 사람처럼 삼각 측량을 통해 멀고 가까움을 인지. 한쪽 눈 보다는 양쪽 눈으로 볼 때 깊이를 더 잘 파악할 수 있는 것처럼, 한 HydraNet이 다른 HydraNet들로부터 이미지의 특징값들을 받아 원근을 측정\n",
        " - 각 카메라로부터 8개의 HydraNet들이 중간 예측치들을 만들어내면, 이 값들을 통합(Stitching up)하는 2차 처리 과정이 진행됨. 통합 과정은 C++과 같은 사람이 짠 코드가 아닌 순환신경망(RNN, Recurrent Neural Network)을 통해 이루어짐. 이러한 전체적인 과정은 마치 거대한 단일 신경망과 같음\n",
        "\n",
        " <img src=\"https://ifh.cc/g/clTMF4.png\" width=\"600px\" height=\"300px\" \n",
        "title=\"8hydranet\" alt=\"background\"></img><br/>\n",
        "\n",
        "\n",
        "* FSD (Full-Self Driving) 컴퓨터\n",
        " - 자율주행 인공지능 추론(Inference) 능력을 위해 차량 내부에 자체 개발한 FSD 컴퓨터를 탑재. GPU와 함께 직접 개발한 NPU가 FSD 칩에 들어가며, 이러한 FSD 칩 2개를 SoC(System on Chip) 형태로 만든 것\n",
        "\n",
        "[출처][1] \n",
        "\n",
        "[1]: https://m.blog.naver.com/shakey7/221932482700 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PJsvOsWlTQG",
        "colab_type": "text"
      },
      "source": [
        "#**학습목표**\n",
        "\n",
        " * 음성인식 기술에서 엔드투엔드방식이 음향 모델(AM)과 언어 모델(LM)을 따로 학습하지 않고 통합해 학습하는 방식과  \n",
        "\n",
        " * 이미지 인식 관련 기술에서도 CNN(Convolutional Neural Network) 모델은 머신러닝의 '특징 추출, 분류' 의 두 단계를  -> '특징 추출과 분류' 의 한 단계로 통합 하는것을 보고  \n",
        "\n",
        "> 인공지능사관학교 멋쟁이사자처럼 교육을 시작으로 앞으로 여러가지 일을 한번에 처리 하는 효율적인 기술에 초점을 맞추고 공부하겠습니다.\n",
        "\n",
        "*  인공지능 관련 기술을 찾아 보면서 현재 여러 기술에 한계가 있는 반면 끊임없이 서로 연결 되어있는 것을 알았습니다.\n",
        " * 언어 인공지능은 음성인식 인공지능과 연결되어 더 편리하고 새로운 기술인 스마트 스피커가 나오고 거기에 이미지 인식 인공지능 까직 더해지면 스마트 자동차 기술까직 이어지는 것을 보았습니다.\n",
        "\n",
        ">서로 연결된다는 생각을 가지며 앞으로 학습하겠습니다.   \n",
        "\n",
        "* 또한 자료를 조사하면서 제일 중요한건 방대한데이터 라는것을 알았습니다.   \n",
        "\n",
        "> 그래서 데이터 수집과 그 데이터의 활용방법 에도 초점을 맞출겁니다.\n",
        " \n",
        "\n",
        ">마지막으로 아직 뭐가 뭔지 잘몰라 오류가 많겠지만 앞으로 배우면서 엘론머스크의   \n",
        " '기술로써 자본을 극복하는 정신' 을 적용하도록 노력하겠습니다."
      ]
    }
  ]
}